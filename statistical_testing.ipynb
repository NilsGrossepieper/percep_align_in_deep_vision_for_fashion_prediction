{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMilZE9NF/ke1dZlB0IZOOi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["experiment_name_statistical_tests = 'Clip16_NIGHTS_Lora_Default_123'\n","vanilla_run = 'Clip16_Vanilla_Default_123'\n","run_one = 'Clip16_NIGHTS_Lora_Default_123_First'\n","run_two = 'Clip16_NIGHTS_Lora_Default_123_Second'\n","run_three = 'Clip16_NIGHTS_Lora_Default_123_Third'\n","test_parameters = {\n","  'paired_bootstrap_pooled': 5000,\n","  'paired_bootstrap_per_product': 5000\n","}"],"metadata":{"id":"lC-OJ2Fm0XCP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import logging\n","import os\n","import sys\n","import yaml\n","\n","PROJECT_ROOT = '/content/drive/MyDrive/perceptual-vits-fashion-forecasting'\n","sys.path.append(os.path.join(PROJECT_ROOT, 'src'))\n","\n","# Logging\n","for handler in logging.root.handlers[:]:\n","    logging.root.removeHandler(handler)\n","logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3687PTyp0CLj","executionInfo":{"status":"ok","timestamp":1762026772336,"user_tz":-60,"elapsed":23191,"user":{"displayName":"Nils Großepieper","userId":"13858288233403889681"}},"outputId":"6619f5fa-0f5a-4f40-b4ad-37140cc62c0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4vhTJsruzRT0","executionInfo":{"status":"ok","timestamp":1762026814458,"user_tz":-60,"elapsed":42102,"user":{"displayName":"Nils Großepieper","userId":"13858288233403889681"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5097c9f1-c3ab-4c30-87f4-209197c7708b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[INFO] NumExpr defaulting to 2 threads.\n","[INFO] Starting Visuelle2 data processing...\n","[INFO] All processed files already exist. Skipping processing.\n","[INFO] Loading and preparing Visuelle2 dataset...\n"]},{"output_type":"stream","name":"stdout","text":["Embedding file found: /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/embeddings/Clip32_NIGHTS_FT_MLP_First_Model_clip_vitb32_mlp_embedding_32_nights_fashion_triplets_embeddings.csv\n"]},{"output_type":"stream","name":"stderr","text":["[INFO] Load training data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_train.csv\n","[INFO] Load validation data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_val.csv\n","[INFO] Load test data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_test.csv\n","[INFO] Load embeddings from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/embeddings/Clip32_NIGHTS_FT_MLP_First_Model_clip_vitb32_mlp_embedding_32_nights_fashion_triplets_embeddings.csv\n","[INFO] Length of training data before merging is 51444 rows.\n","[INFO] Length of training data after merging is 51432 rows.\n","[INFO] Length of validation data before merging is 6408 rows.\n","[INFO] Length of validation data after merging is 6408 rows.\n","[INFO] Length of testing data before merging is 6408 rows.\n","[INFO] Length of testing data after merging is 6408 rows.\n","[INFO] PCA embedding dim: 256\n","[INFO] Loaded data: train=(51432, 378), val=(6408, 378), test=(6408, 378)\n","[INFO] Loading and preparing Visuelle2 dataset...\n"]},{"output_type":"stream","name":"stdout","text":["Embedding file found: /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/embeddings/Clip16_NIGHTS_FT_MLP_First_Model_clip_vitb16_mlp_embedding_16_nights_fashion_triplets_embeddings.csv\n"]},{"output_type":"stream","name":"stderr","text":["[INFO] Load training data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_train.csv\n","[INFO] Load validation data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_val.csv\n","[INFO] Load test data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_test.csv\n","[INFO] Load embeddings from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/embeddings/Clip16_NIGHTS_FT_MLP_First_Model_clip_vitb16_mlp_embedding_16_nights_fashion_triplets_embeddings.csv\n","[INFO] Length of training data before merging is 51444 rows.\n","[INFO] Length of training data after merging is 51432 rows.\n","[INFO] Length of validation data before merging is 6408 rows.\n","[INFO] Length of validation data after merging is 6408 rows.\n","[INFO] Length of testing data before merging is 6408 rows.\n","[INFO] Length of testing data after merging is 6408 rows.\n","[INFO] PCA embedding dim: 256\n","[INFO] Loaded data: train=(51432, 378), val=(6408, 378), test=(6408, 378)\n","[INFO] Loading and preparing Visuelle2 dataset...\n"]},{"output_type":"stream","name":"stdout","text":["Embedding file found: /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/embeddings/Clip16_NIGHTS_FT_MLP_Second_Model_clip_vitb16_mlp_embedding_16_nights_fashion_triplets_embeddings.csv\n"]},{"output_type":"stream","name":"stderr","text":["[INFO] Load training data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_train.csv\n","[INFO] Load validation data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_val.csv\n","[INFO] Load test data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_test.csv\n","[INFO] Load embeddings from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/embeddings/Clip16_NIGHTS_FT_MLP_Second_Model_clip_vitb16_mlp_embedding_16_nights_fashion_triplets_embeddings.csv\n","[INFO] Length of training data before merging is 51444 rows.\n","[INFO] Length of training data after merging is 51432 rows.\n","[INFO] Length of validation data before merging is 6408 rows.\n","[INFO] Length of validation data after merging is 6408 rows.\n","[INFO] Length of testing data before merging is 6408 rows.\n","[INFO] Length of testing data after merging is 6408 rows.\n","[INFO] PCA embedding dim: 256\n","[INFO] Loaded data: train=(51432, 378), val=(6408, 378), test=(6408, 378)\n","[INFO] Loading and preparing Visuelle2 dataset...\n"]},{"output_type":"stream","name":"stdout","text":["Embedding file found: /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/embeddings/Clip16_NIGHTS_FT_MLP_Third_Model_clip_vitb16_mlp_embedding_16_nights_fashion_triplets_embeddings.csv\n"]},{"output_type":"stream","name":"stderr","text":["[INFO] Load training data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_train.csv\n","[INFO] Load validation data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_val.csv\n","[INFO] Load test data from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/processed_data/melt_true_aw19_default_split/visuelle2_test.csv\n","[INFO] Load embeddings from /content/drive/MyDrive/perceptual-vits-fashion-forecasting/datasets/visuelle2/embeddings/Clip16_NIGHTS_FT_MLP_Third_Model_clip_vitb16_mlp_embedding_16_nights_fashion_triplets_embeddings.csv\n","[INFO] Length of training data before merging is 51444 rows.\n","[INFO] Length of training data after merging is 51432 rows.\n","[INFO] Length of validation data before merging is 6408 rows.\n","[INFO] Length of validation data after merging is 6408 rows.\n","[INFO] Length of testing data before merging is 6408 rows.\n","[INFO] Length of testing data after merging is 6408 rows.\n","[INFO] PCA embedding dim: 256\n","[INFO] Loaded data: train=(51432, 378), val=(6408, 378), test=(6408, 378)\n"]}],"source":["# Load config_vanilla\n","config_vanilla_path = os.path.join(PROJECT_ROOT, 'configs', f\"{vanilla_run}.yaml\")\n","if not os.path.isfile(config_vanilla_path):\n","    raise FileNotFoundError(f\"config_vanilla file not found: {config_vanilla_path}\")\n","\n","with open(config_vanilla_path, 'r') as file:\n","    config_vanilla = yaml.safe_load(file)\n","\n","\n","from data.load_visuelle import load_visuelle\n","from data.process_visuelle import process_visuelle\n","\n","# Process Visuelle2 data\n","results_process_visuelle = process_visuelle(\n","    season=config_vanilla['data']['season'],\n","    split_method=config_vanilla['data']['split_method'],\n","    melt_data=config_vanilla['global']['melt_data'],\n","    project_root=config_vanilla['global']['project_root']\n",")\n","\n","# Load the embedding files for vanilla model\n","embeddings_dir = os.path.join(config_vanilla['global']['project_root'], 'datasets', 'visuelle2', 'embeddings')\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","if config_vanilla['vision_model']['model_family'] == 'vit':\n","  experiment_name = f\"{str(config_vanilla['vision_model']['tag'])}_{str(config_vanilla['vision_model']['model_type'])}_{str(config_vanilla['vision_model']['training_method'])}_{str(config_vanilla['vision_model']['vit']['feat_type'])}_{str(config_vanilla['vision_model']['vit']['stride'])}_{str(config_vanilla['vision_model']['dataset_name'])}\"\n","elif config_vanilla['vision_model']['model_family']  =='cnn':\n","  experiment_name = f\"{str(config_vanilla['vision_model']['tag'])}_{str(config_vanilla['vision_model']['model_type'])}_{str(config_vanilla['vision_model']['training_method'])}_{str(config_vanilla['vision_model']['dataset_name'])}\"\n","\n","embedding_filename = f'{experiment_name}_embeddings.csv'\n","emb_filepath_vanilla = os.path.join(embeddings_dir, embedding_filename)\n","\n","if not os.path.isfile(emb_filepath_vanilla):\n","  raise FileNotFoundError(f\"Embedding file not found: {emb_filepath_vanilla}\")\n","else:\n","  print(f\"Embedding file found: {emb_filepath_vanilla}\")\n","\n","\n","# Load dataset with embeddings\n","dict_load_vanilla = load_visuelle(\n","    process_data_dict=results_process_visuelle,\n","    embedding_path=emb_filepath_vanilla,\n","    using_year_int=config_vanilla['features']['using_year_int'],\n","    using_year_dummies=config_vanilla['features']['using_year_dummies'],\n","    using_season_dummies=config_vanilla['features']['using_season_dummies'],\n","    using_price_float=config_vanilla['features']['using_price_float'],\n","    using_category_dummies=config_vanilla['features']['using_category_dummies'],\n","    using_color_dummies=config_vanilla['features']['using_color_dummies'],\n","    using_fabric_dummies=config_vanilla['features']['using_fabric_dummies'],\n","    using_store_int=config_vanilla['features']['using_store_int'],\n","    using_store_dummies=config_vanilla['features']['using_store_dummies'],\n","    using_week_dummies=config_vanilla['features']['using_week_dummies'],\n","    pca=config_vanilla['features']['pca'],\n","    n_components=config_vanilla['features']['n_components'],\n","    visualize_pca=config_vanilla['features']['visualize_pca'],\n","    melt_data=config_vanilla['global']['melt_data'],\n","    dummy_normalization=config_vanilla['forecasting']['knn']['dummy_normalization'],\n","    normalize_embeddings_manually=config_vanilla['forecasting']['knn']['normalize_embeddings_manually'],\n","    project_root=config_vanilla['global']['project_root']\n",")\n","\n","\n","# Load config_run_one\n","config_one_path = os.path.join(PROJECT_ROOT, 'configs', f\"{run_one}.yaml\")\n","if not os.path.isfile(config_one_path):\n","    raise FileNotFoundError(f\"config_one file not found: {config_one_path}\")\n","\n","with open(config_one_path, 'r') as file:\n","    config_one = yaml.safe_load(file)\n","\n","\n","# Load the embedding files for first run model\n","embeddings_dir = os.path.join(config_one['global']['project_root'], 'datasets', 'visuelle2', 'embeddings')\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","if config_one['vision_model']['model_family'] == 'vit':\n","  experiment_name = f\"{str(config_one['vision_model']['tag'])}_{str(config_one['vision_model']['model_type'])}_{str(config_one['vision_model']['training_method'])}_{str(config_one['vision_model']['vit']['feat_type'])}_{str(config_one['vision_model']['vit']['stride'])}_{str(config_one['vision_model']['dataset_name'])}\"\n","elif config_one['vision_model']['model_family']  =='cnn':\n","  experiment_name = f\"{str(config_one['vision_model']['tag'])}_{str(config_one['vision_model']['model_type'])}_{str(config_one['vision_model']['training_method'])}_{str(config_one['vision_model']['dataset_name'])}\"\n","\n","embedding_filename = f'{experiment_name}_embeddings.csv'\n","emb_filepath_run_one = os.path.join(embeddings_dir, embedding_filename)\n","\n","if not os.path.isfile(emb_filepath_run_one):\n","  raise FileNotFoundError(f\"Embedding file not found: {emb_filepath_run_one}\")\n","else:\n","  print(f\"Embedding file found: {emb_filepath_run_one}\")\n","\n","\n","# Load dataset with embeddings\n","dict_load_run_one = load_visuelle(\n","    process_data_dict=results_process_visuelle,\n","    embedding_path=emb_filepath_run_one,\n","    using_year_int=config_one['features']['using_year_int'],\n","    using_year_dummies=config_one['features']['using_year_dummies'],\n","    using_season_dummies=config_one['features']['using_season_dummies'],\n","    using_price_float=config_one['features']['using_price_float'],\n","    using_category_dummies=config_one['features']['using_category_dummies'],\n","    using_color_dummies=config_one['features']['using_color_dummies'],\n","    using_fabric_dummies=config_one['features']['using_fabric_dummies'],\n","    using_store_int=config_one['features']['using_store_int'],\n","    using_store_dummies=config_one['features']['using_store_dummies'],\n","    using_week_dummies=config_one['features']['using_week_dummies'],\n","    pca=config_one['features']['pca'],\n","    n_components=config_one['features']['n_components'],\n","    visualize_pca=config_one['features']['visualize_pca'],\n","    melt_data=config_one['global']['melt_data'],\n","    dummy_normalization=config_one['forecasting']['knn']['dummy_normalization'],\n","    normalize_embeddings_manually=config_one['forecasting']['knn']['normalize_embeddings_manually'],\n","    project_root=config_one['global']['project_root']\n",")\n","\n","\n","# Load config_run_two\n","config_two_path = os.path.join(PROJECT_ROOT, 'configs', f\"{run_two}.yaml\")\n","if not os.path.isfile(config_two_path):\n","    raise FileNotFoundError(f\"config_two file not found: {config_two_path}\")\n","\n","with open(config_two_path, 'r') as file:\n","    config_two = yaml.safe_load(file)\n","\n","\n","# Load the embedding files for first run model\n","embeddings_dir = os.path.join(config_two['global']['project_root'], 'datasets', 'visuelle2', 'embeddings')\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","if config_two['vision_model']['model_family'] == 'vit':\n","  experiment_name = f\"{str(config_two['vision_model']['tag'])}_{str(config_two['vision_model']['model_type'])}_{str(config_two['vision_model']['training_method'])}_{str(config_two['vision_model']['vit']['feat_type'])}_{str(config_two['vision_model']['vit']['stride'])}_{str(config_two['vision_model']['dataset_name'])}\"\n","elif config_two['vision_model']['model_family']  =='cnn':\n","  experiment_name = f\"{str(config_two['vision_model']['tag'])}_{str(config_two['vision_model']['model_type'])}_{str(config_two['vision_model']['training_method'])}_{str(config_two['vision_model']['dataset_name'])}\"\n","\n","embedding_filename = f'{experiment_name}_embeddings.csv'\n","emb_filepath_run_two = os.path.join(embeddings_dir, embedding_filename)\n","\n","if not os.path.isfile(emb_filepath_run_two):\n","  raise FileNotFoundError(f\"Embedding file not found: {emb_filepath_run_two}\")\n","else:\n","  print(f\"Embedding file found: {emb_filepath_run_two}\")\n","\n","\n","# Load dataset with embeddings\n","dict_load_run_two = load_visuelle(\n","    process_data_dict=results_process_visuelle,\n","    embedding_path=emb_filepath_run_two,\n","    using_year_int=config_two['features']['using_year_int'],\n","    using_year_dummies=config_two['features']['using_year_dummies'],\n","    using_season_dummies=config_two['features']['using_season_dummies'],\n","    using_price_float=config_two['features']['using_price_float'],\n","    using_category_dummies=config_two['features']['using_category_dummies'],\n","    using_color_dummies=config_two['features']['using_color_dummies'],\n","    using_fabric_dummies=config_two['features']['using_fabric_dummies'],\n","    using_store_int=config_two['features']['using_store_int'],\n","    using_store_dummies=config_two['features']['using_store_dummies'],\n","    using_week_dummies=config_two['features']['using_week_dummies'],\n","    pca=config_two['features']['pca'],\n","    n_components=config_two['features']['n_components'],\n","    visualize_pca=config_two['features']['visualize_pca'],\n","    melt_data=config_two['global']['melt_data'],\n","    dummy_normalization=config_two['forecasting']['knn']['dummy_normalization'],\n","    normalize_embeddings_manually=config_two['forecasting']['knn']['normalize_embeddings_manually'],\n","    project_root=config_two['global']['project_root']\n",")\n","\n","\n","# Load config_run_three\n","config_three_path = os.path.join(PROJECT_ROOT, 'configs', f\"{run_three}.yaml\")\n","if not os.path.isfile(config_three_path):\n","    raise FileNotFoundError(f\"config_three file not found: {config_three_path}\")\n","\n","with open(config_three_path, 'r') as file:\n","    config_three = yaml.safe_load(file)\n","\n","\n","# Load the embedding files for first run model\n","embeddings_dir = os.path.join(config_three['global']['project_root'], 'datasets', 'visuelle2', 'embeddings')\n","os.makedirs(embeddings_dir, exist_ok=True)\n","\n","if config_three['vision_model']['model_family'] == 'vit':\n","  experiment_name = f\"{str(config_three['vision_model']['tag'])}_{str(config_three['vision_model']['model_type'])}_{str(config_three['vision_model']['training_method'])}_{str(config_three['vision_model']['vit']['feat_type'])}_{str(config_three['vision_model']['vit']['stride'])}_{str(config_three['vision_model']['dataset_name'])}\"\n","elif config_three['vision_model']['model_family']  =='cnn':\n","  experiment_name = f\"{str(config_three['vision_model']['tag'])}_{str(config_three['vision_model']['model_type'])}_{str(config_three['vision_model']['training_method'])}_{str(config_three['vision_model']['dataset_name'])}\"\n","\n","embedding_filename = f'{experiment_name}_embeddings.csv'\n","emb_filepath_run_three = os.path.join(embeddings_dir, embedding_filename)\n","\n","if not os.path.isfile(emb_filepath_run_three):\n","  raise FileNotFoundError(f\"Embedding file not found: {emb_filepath_run_three}\")\n","else:\n","  print(f\"Embedding file found: {emb_filepath_run_three}\")\n","\n","\n","# Load dataset with embeddings\n","dict_load_run_three = load_visuelle(\n","    process_data_dict=results_process_visuelle,\n","    embedding_path=emb_filepath_run_three,\n","    using_year_int=config_three['features']['using_year_int'],\n","    using_year_dummies=config_three['features']['using_year_dummies'],\n","    using_season_dummies=config_three['features']['using_season_dummies'],\n","    using_price_float=config_three['features']['using_price_float'],\n","    using_category_dummies=config_three['features']['using_category_dummies'],\n","    using_color_dummies=config_three['features']['using_color_dummies'],\n","    using_fabric_dummies=config_three['features']['using_fabric_dummies'],\n","    using_store_int=config_three['features']['using_store_int'],\n","    using_store_dummies=config_three['features']['using_store_dummies'],\n","    using_week_dummies=config_three['features']['using_week_dummies'],\n","    pca=config_three['features']['pca'],\n","    n_components=config_three['features']['n_components'],\n","    visualize_pca=config_three['features']['visualize_pca'],\n","    melt_data=config_three['global']['melt_data'],\n","    dummy_normalization=config_three['forecasting']['knn']['dummy_normalization'],\n","    normalize_embeddings_manually=config_three['forecasting']['knn']['normalize_embeddings_manually'],\n","    project_root=config_three['global']['project_root']\n",")"]},{"cell_type":"code","source":["from utils_prediction.statistical_tests import test_for_significance\n","\n","test_for_significance(\n","    dict_load_vanilla = dict_load_vanilla,\n","    dict_load_run_one = dict_load_run_one,\n","    dict_load_run_two = dict_load_run_two,\n","    dict_load_run_three = dict_load_run_three,\n","    experiment_name = experiment_name_statistical_tests,\n","    vanilla_run_name = vanilla_run,\n","    run_one_name = run_one,\n","    run_two_name = run_two,\n","    run_three_name = run_three,\n","    test_parameters = test_parameters,\n","    random_seed = 123,\n","    project_root = PROJECT_ROOT,\n","    scalar = 53.0)"],"metadata":{"id":"8DQDfwdK0bvj","executionInfo":{"status":"ok","timestamp":1762028495202,"user_tz":-60,"elapsed":1680739,"user":{"displayName":"Nils Großepieper","userId":"13858288233403889681"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"79733f88-26e1-4dab-e89f-a9e610b2a000"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[INFO] Testing for significance...\n","[INFO] Saved global results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/global_results_gradient_boosting_regressor_model.csv\n","[INFO] Saved paired bootstrap pooled results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/paired_bootstrap_pooled_gradient_boosting_regressor_model.csv\n","[INFO] Saved paired bootstrap per-product results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/paired_bootstrap_per_product_gradient_boosting_regressor_model.csv\n","[INFO] Saved Wilcoxon per-product results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/wilcoxon_per_product_gradient_boosting_regressor_model.csv\n","[INFO] Saved weekly vanilla results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/weekly_vanilla_results_gradient_boosting_regressor_model.csv\n","[INFO] Saved weekly trained results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/weekly_trained_results_gradient_boosting_regressor_model.csv\n","[INFO] Saved global results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/global_results_knn_regressor_model.csv\n","[INFO] Saved paired bootstrap pooled results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/paired_bootstrap_pooled_knn_regressor_model.csv\n","[INFO] Saved paired bootstrap per-product results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/paired_bootstrap_per_product_knn_regressor_model.csv\n","[INFO] Saved Wilcoxon per-product results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/wilcoxon_per_product_knn_regressor_model.csv\n","[INFO] Saved weekly vanilla results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/weekly_vanilla_results_knn_regressor_model.csv\n","[INFO] Saved weekly trained results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/weekly_trained_results_knn_regressor_model.csv\n","[INFO] Saved global results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/global_results_random_forest_regressor_model.csv\n","[INFO] Saved paired bootstrap pooled results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/paired_bootstrap_pooled_random_forest_regressor_model.csv\n","[INFO] Saved paired bootstrap per-product results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/paired_bootstrap_per_product_random_forest_regressor_model.csv\n","[INFO] Saved Wilcoxon per-product results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/wilcoxon_per_product_random_forest_regressor_model.csv\n","[INFO] Saved weekly vanilla results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/weekly_vanilla_results_random_forest_regressor_model.csv\n","[INFO] Saved weekly trained results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/weekly_trained_results_random_forest_regressor_model.csv\n","[INFO] Saved global results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/global_results_ridge_regressor_model.csv\n","[INFO] Saved paired bootstrap pooled results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/paired_bootstrap_pooled_ridge_regressor_model.csv\n","[INFO] Saved paired bootstrap per-product results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/paired_bootstrap_per_product_ridge_regressor_model.csv\n","[INFO] Saved Wilcoxon per-product results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/wilcoxon_per_product_ridge_regressor_model.csv\n","[INFO] Saved weekly vanilla results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/weekly_vanilla_results_ridge_regressor_model.csv\n","[INFO] Saved weekly trained results at /content/drive/MyDrive/perceptual-vits-fashion-forecasting/experiments/statistical_tests/Is CLIP 16 better than CLIP 32 in NIGHTS/weekly_trained_results_ridge_regressor_model.csv\n"]}]}]}